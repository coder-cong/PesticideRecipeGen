digraph {
	graph [size="49.949999999999996,49.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140556717992576 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	140556718007296 [label=AddmmBackward]
	140556718007392 -> 140556718007296
	140556717992064 [label="block.8.bias
 (10)" fillcolor=lightblue]
	140556717992064 -> 140556718007392
	140556718007392 [label=AccumulateGrad]
	140556718007344 -> 140556718007296
	140556718007344 [label=ViewBackward]
	140556718007200 -> 140556718007344
	140556718007200 [label=MeanBackward1]
	140556718007536 -> 140556718007200
	140556718007536 [label=ReluBackward0]
	140556718007632 -> 140556718007536
	140556718007632 [label=AddBackward0]
	140556718007728 -> 140556718007632
	140556718007728 [label=NativeBatchNormBackward]
	140556718007872 -> 140556718007728
	140556718007872 [label=MkldnnConvolutionBackward]
	140556718008064 -> 140556718007872
	140556718008064 [label=ReluBackward0]
	140556718008256 -> 140556718008064
	140556718008256 [label=NativeBatchNormBackward]
	140556718008352 -> 140556718008256
	140556718008352 [label=MkldnnConvolutionBackward]
	140556718007680 -> 140556718008352
	140556718007680 [label=ReluBackward0]
	140556718008688 -> 140556718007680
	140556718008688 [label=AddBackward0]
	140556718008784 -> 140556718008688
	140556718008784 [label=NativeBatchNormBackward]
	140556718008928 -> 140556718008784
	140556718008928 [label=MkldnnConvolutionBackward]
	140556718009120 -> 140556718008928
	140556718009120 [label=ReluBackward0]
	140556718009312 -> 140556718009120
	140556718009312 [label=NativeBatchNormBackward]
	140556718009408 -> 140556718009312
	140556718009408 [label=MkldnnConvolutionBackward]
	140556718009600 -> 140556718009408
	140556718009600 [label=ReluBackward0]
	140556718009792 -> 140556718009600
	140556718009792 [label=AddBackward0]
	140556718009888 -> 140556718009792
	140556718009888 [label=NativeBatchNormBackward]
	140556718010032 -> 140556718009888
	140556718010032 [label=MkldnnConvolutionBackward]
	140556718010224 -> 140556718010032
	140556718010224 [label=ReluBackward0]
	140556718010320 -> 140556718010224
	140556718010320 [label=NativeBatchNormBackward]
	140556718067920 -> 140556718010320
	140556718067920 [label=MkldnnConvolutionBackward]
	140556718009840 -> 140556718067920
	140556718009840 [label=ReluBackward0]
	140556718068256 -> 140556718009840
	140556718068256 [label=AddBackward0]
	140556718068352 -> 140556718068256
	140556718068352 [label=NativeBatchNormBackward]
	140556718068496 -> 140556718068352
	140556718068496 [label=MkldnnConvolutionBackward]
	140556718068688 -> 140556718068496
	140556718068688 [label=ReluBackward0]
	140556718068880 -> 140556718068688
	140556718068880 [label=NativeBatchNormBackward]
	140556718068976 -> 140556718068880
	140556718068976 [label=MkldnnConvolutionBackward]
	140556718069168 -> 140556718068976
	140556718069168 [label=ReluBackward0]
	140556718069360 -> 140556718069168
	140556718069360 [label=AddBackward0]
	140556718069456 -> 140556718069360
	140556718069456 [label=NativeBatchNormBackward]
	140556718069600 -> 140556718069456
	140556718069600 [label=MkldnnConvolutionBackward]
	140556718069792 -> 140556718069600
	140556718069792 [label=ReluBackward0]
	140556718069984 -> 140556718069792
	140556718069984 [label=NativeBatchNormBackward]
	140556718070080 -> 140556718069984
	140556718070080 [label=MkldnnConvolutionBackward]
	140556718069408 -> 140556718070080
	140556718069408 [label=ReluBackward0]
	140556718070416 -> 140556718069408
	140556718070416 [label=AddBackward0]
	140556718070512 -> 140556718070416
	140556718070512 [label=NativeBatchNormBackward]
	140556718070656 -> 140556718070512
	140556718070656 [label=MkldnnConvolutionBackward]
	140556718070848 -> 140556718070656
	140556718070848 [label=ReluBackward0]
	140556718071040 -> 140556718070848
	140556718071040 [label=NativeBatchNormBackward]
	140556718071136 -> 140556718071040
	140556718071136 [label=MkldnnConvolutionBackward]
	140556718071328 -> 140556718071136
	140556717993280 [label="输入
 (1, 3, 224, 224)" fillcolor=lightblue]
	140556717993280 -> 140556718071328
	140556718071328 [label=AccumulateGrad]
	140556718071280 -> 140556718071136
	140556718376512 [label="block.0.blocks.0.weight
 (128, 3, 3, 3)" fillcolor=lightblue]
	140556718376512 -> 140556718071280
	140556718071280 [label=AccumulateGrad]
	140556718071232 -> 140556718071136
	140556718376640 [label="block.0.blocks.0.bias
 (128)" fillcolor=lightblue]
	140556718376640 -> 140556718071232
	140556718071232 [label=AccumulateGrad]
	140556718071088 -> 140556718071040
	140556718376832 [label="block.0.blocks.1.weight
 (128)" fillcolor=lightblue]
	140556718376832 -> 140556718071088
	140556718071088 [label=AccumulateGrad]
	140556718070944 -> 140556718071040
	140556718376960 [label="block.0.blocks.1.bias
 (128)" fillcolor=lightblue]
	140556718376960 -> 140556718070944
	140556718070944 [label=AccumulateGrad]
	140556718070800 -> 140556718070656
	140556718377408 [label="block.0.blocks.Conv_out128.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140556718377408 -> 140556718070800
	140556718070800 [label=AccumulateGrad]
	140556718070752 -> 140556718070656
	140556718377600 [label="block.0.blocks.Conv_out128.bias
 (128)" fillcolor=lightblue]
	140556718377600 -> 140556718070752
	140556718070752 [label=AccumulateGrad]
	140556718070608 -> 140556718070512
	140556718377792 [label="block.0.blocks.BachNorm.weight
 (128)" fillcolor=lightblue]
	140556718377792 -> 140556718070608
	140556718070608 [label=AccumulateGrad]
	140556718070560 -> 140556718070512
	140556718377920 [label="block.0.blocks.BachNorm.bias
 (128)" fillcolor=lightblue]
	140556718377920 -> 140556718070560
	140556718070560 [label=AccumulateGrad]
	140556718070464 -> 140556718070416
	140556718070464 [label=MkldnnConvolutionBackward]
	140556718071328 -> 140556718070464
	140556718070992 -> 140556718070464
	140556718377664 [label="block.0.onexone.weight
 (128, 3, 1, 1)" fillcolor=lightblue]
	140556718377664 -> 140556718070992
	140556718070992 [label=AccumulateGrad]
	140556718070896 -> 140556718070464
	140556718378240 [label="block.0.onexone.bias
 (128)" fillcolor=lightblue]
	140556718378240 -> 140556718070896
	140556718070896 [label=AccumulateGrad]
	140556718070272 -> 140556718070080
	140556718376256 [label="block.1.blocks.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140556718376256 -> 140556718070272
	140556718070272 [label=AccumulateGrad]
	140556718070224 -> 140556718070080
	140556718378304 [label="block.1.blocks.0.bias
 (128)" fillcolor=lightblue]
	140556718378304 -> 140556718070224
	140556718070224 [label=AccumulateGrad]
	140556718070032 -> 140556718069984
	140556718378496 [label="block.1.blocks.1.weight
 (128)" fillcolor=lightblue]
	140556718378496 -> 140556718070032
	140556718070032 [label=AccumulateGrad]
	140556718069888 -> 140556718069984
	140556718378624 [label="block.1.blocks.1.bias
 (128)" fillcolor=lightblue]
	140556718378624 -> 140556718069888
	140556718069888 [label=AccumulateGrad]
	140556718069744 -> 140556718069600
	140556717908224 [label="block.1.blocks.Conv_out128.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140556717908224 -> 140556718069744
	140556718069744 [label=AccumulateGrad]
	140556718069696 -> 140556718069600
	140556717908352 [label="block.1.blocks.Conv_out128.bias
 (128)" fillcolor=lightblue]
	140556717908352 -> 140556718069696
	140556718069696 [label=AccumulateGrad]
	140556718069552 -> 140556718069456
	140556717908608 [label="block.1.blocks.BachNorm.weight
 (128)" fillcolor=lightblue]
	140556717908608 -> 140556718069552
	140556718069552 [label=AccumulateGrad]
	140556718069504 -> 140556718069456
	140556717908736 [label="block.1.blocks.BachNorm.bias
 (128)" fillcolor=lightblue]
	140556717908736 -> 140556718069504
	140556718069504 [label=AccumulateGrad]
	140556718069408 -> 140556718069360
	140556718069120 -> 140556718068976
	140556717909056 [label="block.2.blocks.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140556717909056 -> 140556718069120
	140556718069120 [label=AccumulateGrad]
	140556718069072 -> 140556718068976
	140556717909184 [label="block.2.blocks.0.bias
 (256)" fillcolor=lightblue]
	140556717909184 -> 140556718069072
	140556718069072 [label=AccumulateGrad]
	140556718068928 -> 140556718068880
	140556717909504 [label="block.2.blocks.1.weight
 (256)" fillcolor=lightblue]
	140556717909504 -> 140556718068928
	140556718068928 [label=AccumulateGrad]
	140556718068784 -> 140556718068880
	140556717909632 [label="block.2.blocks.1.bias
 (256)" fillcolor=lightblue]
	140556717909632 -> 140556718068784
	140556718068784 [label=AccumulateGrad]
	140556718068640 -> 140556718068496
	140556717910336 [label="block.2.blocks.Conv_out256.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140556717910336 -> 140556718068640
	140556718068640 [label=AccumulateGrad]
	140556718068592 -> 140556718068496
	140556717910464 [label="block.2.blocks.Conv_out256.bias
 (256)" fillcolor=lightblue]
	140556717910464 -> 140556718068592
	140556718068592 [label=AccumulateGrad]
	140556718068448 -> 140556718068352
	140556717910784 [label="block.2.blocks.BachNorm.weight
 (256)" fillcolor=lightblue]
	140556717910784 -> 140556718068448
	140556718068448 [label=AccumulateGrad]
	140556718068400 -> 140556718068352
	140556717910912 [label="block.2.blocks.BachNorm.bias
 (256)" fillcolor=lightblue]
	140556717910912 -> 140556718068400
	140556718068400 [label=AccumulateGrad]
	140556718068304 -> 140556718068256
	140556718068304 [label=MkldnnConvolutionBackward]
	140556718069168 -> 140556718068304
	140556718068832 -> 140556718068304
	140556717910656 [label="block.2.onexone.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140556717910656 -> 140556718068832
	140556718068832 [label=AccumulateGrad]
	140556718068736 -> 140556718068304
	140556717911296 [label="block.2.onexone.bias
 (256)" fillcolor=lightblue]
	140556717911296 -> 140556718068736
	140556718068736 [label=AccumulateGrad]
	140556718068112 -> 140556718067920
	140556717911424 [label="block.3.blocks.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140556717911424 -> 140556718068112
	140556718068112 [label=AccumulateGrad]
	140556718068064 -> 140556718067920
	140556717911552 [label="block.3.blocks.0.bias
 (256)" fillcolor=lightblue]
	140556717911552 -> 140556718068064
	140556718068064 [label=AccumulateGrad]
	140556718067872 -> 140556718010320
	140556717911808 [label="block.3.blocks.1.weight
 (256)" fillcolor=lightblue]
	140556717911808 -> 140556718067872
	140556718067872 [label=AccumulateGrad]
	140556718067776 -> 140556718010320
	140556717911936 [label="block.3.blocks.1.bias
 (256)" fillcolor=lightblue]
	140556717911936 -> 140556718067776
	140556718067776 [label=AccumulateGrad]
	140556718010176 -> 140556718010032
	140556717949568 [label="block.3.blocks.Conv_out256.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140556717949568 -> 140556718010176
	140556718010176 [label=AccumulateGrad]
	140556718010128 -> 140556718010032
	140556717949696 [label="block.3.blocks.Conv_out256.bias
 (256)" fillcolor=lightblue]
	140556717949696 -> 140556718010128
	140556718010128 [label=AccumulateGrad]
	140556718009984 -> 140556718009888
	140556717950016 [label="block.3.blocks.BachNorm.weight
 (256)" fillcolor=lightblue]
	140556717950016 -> 140556718009984
	140556718009984 [label=AccumulateGrad]
	140556718009936 -> 140556718009888
	140556717950144 [label="block.3.blocks.BachNorm.bias
 (256)" fillcolor=lightblue]
	140556717950144 -> 140556718009936
	140556718009936 [label=AccumulateGrad]
	140556718009840 -> 140556718009792
	140556718009552 -> 140556718009408
	140556717950464 [label="block.4.blocks.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140556717950464 -> 140556718009552
	140556718009552 [label=AccumulateGrad]
	140556718009504 -> 140556718009408
	140556717950592 [label="block.4.blocks.0.bias
 (512)" fillcolor=lightblue]
	140556717950592 -> 140556718009504
	140556718009504 [label=AccumulateGrad]
	140556718009360 -> 140556718009312
	140556717950912 [label="block.4.blocks.1.weight
 (512)" fillcolor=lightblue]
	140556717950912 -> 140556718009360
	140556718009360 [label=AccumulateGrad]
	140556718009216 -> 140556718009312
	140556717951040 [label="block.4.blocks.1.bias
 (512)" fillcolor=lightblue]
	140556717951040 -> 140556718009216
	140556718009216 [label=AccumulateGrad]
	140556718009072 -> 140556718008928
	140556717951744 [label="block.4.blocks.Conv_out512.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140556717951744 -> 140556718009072
	140556718009072 [label=AccumulateGrad]
	140556718009024 -> 140556718008928
	140556717951872 [label="block.4.blocks.Conv_out512.bias
 (512)" fillcolor=lightblue]
	140556717951872 -> 140556718009024
	140556718009024 [label=AccumulateGrad]
	140556718008880 -> 140556718008784
	140556717952192 [label="block.4.blocks.BachNorm.weight
 (512)" fillcolor=lightblue]
	140556717952192 -> 140556718008880
	140556718008880 [label=AccumulateGrad]
	140556718008832 -> 140556718008784
	140556717952320 [label="block.4.blocks.BachNorm.bias
 (512)" fillcolor=lightblue]
	140556717952320 -> 140556718008832
	140556718008832 [label=AccumulateGrad]
	140556718008736 -> 140556718008688
	140556718008736 [label=MkldnnConvolutionBackward]
	140556718009600 -> 140556718008736
	140556718009264 -> 140556718008736
	140556717952832 [label="block.4.onexone.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140556717952832 -> 140556718009264
	140556718009264 [label=AccumulateGrad]
	140556718009168 -> 140556718008736
	140556717952960 [label="block.4.onexone.bias
 (512)" fillcolor=lightblue]
	140556717952960 -> 140556718009168
	140556718009168 [label=AccumulateGrad]
	140556718008544 -> 140556718008352
	140556717990272 [label="block.5.blocks.0.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140556717990272 -> 140556718008544
	140556718008544 [label=AccumulateGrad]
	140556718008496 -> 140556718008352
	140556717990400 [label="block.5.blocks.0.bias
 (512)" fillcolor=lightblue]
	140556717990400 -> 140556718008496
	140556718008496 [label=AccumulateGrad]
	140556718008304 -> 140556718008256
	140556717990656 [label="block.5.blocks.1.weight
 (512)" fillcolor=lightblue]
	140556717990656 -> 140556718008304
	140556718008304 [label=AccumulateGrad]
	140556718008160 -> 140556718008256
	140556717990784 [label="block.5.blocks.1.bias
 (512)" fillcolor=lightblue]
	140556717990784 -> 140556718008160
	140556718008160 [label=AccumulateGrad]
	140556718008016 -> 140556718007872
	140556717991744 [label="block.5.blocks.Conv_out512.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140556717991744 -> 140556718008016
	140556718008016 [label=AccumulateGrad]
	140556718007968 -> 140556718007872
	140556717991872 [label="block.5.blocks.Conv_out512.bias
 (512)" fillcolor=lightblue]
	140556717991872 -> 140556718007968
	140556718007968 [label=AccumulateGrad]
	140556718007824 -> 140556718007728
	140556717992192 [label="block.5.blocks.BachNorm.weight
 (512)" fillcolor=lightblue]
	140556717992192 -> 140556718007824
	140556718007824 [label=AccumulateGrad]
	140556718007776 -> 140556718007728
	140556717992320 [label="block.5.blocks.BachNorm.bias
 (512)" fillcolor=lightblue]
	140556717992320 -> 140556718007776
	140556718007776 [label=AccumulateGrad]
	140556718007680 -> 140556718007632
	140556718007248 -> 140556718007296
	140556718007248 [label=TBackward]
	140556718007584 -> 140556718007248
	140556772097408 [label="block.8.weight
 (10, 512)" fillcolor=lightblue]
	140556772097408 -> 140556718007584
	140556718007584 [label=AccumulateGrad]
	140556718007296 -> 140556717992576
}
